Attention Gate 구조 설명   

paper: **Attention U-Net: Learning Where to Look for the Pancreas**

Figure 2는 제안된 Additive Attention Gate의 구조를 보여줍니다. 
* 이 게이트의 목적은 입력 특징(x) 중에서 어떤 부분을 중요하게 보고 다음 단계로 넘길지를 '문맥 정보(g)'를 이용하여 결정하는 것입니다.
![image](https://github.com/user-attachments/assets/fbe488ed-61b3-46b5-9445-5c3b097fc3b6)



1.  **입력 (Inputs):**
    * **`x^l`**: U-Net의 **Skip Connection**을 통해 전달되는 특징 맵(feature map)입니다. 이는 인코더(Encoder, U-Net의 앞부분)의 `l`번째 레이어에서 온 정보로, 저수준(low-level) 특징을 담고 있습니다. (`F_l x H_x x W_x x D_x` 크기)
    * **`g`**: U-Net의 **디코더(Decoder, U-Net의 뒷부분)에서 한 단계 더 깊은(coarser scale) 곳**에서 오는 **Gating Signal**입니다. 이 신호는 더 넓은 영역의 정보를 함축하고 있어, 현재 `x^l`이 어디에 집중해야 하는지에 대한 **Contextual Information**를 제공합니다.
      * (`F_g x H_g x W_g x D_g` 크기)

2.  **처리 과정 (Processing Steps):**
    * **선형 변환 (Linear Transformations, `W_x`, `W_g`):** `x^l`과 `g`는 각각 **1x1x1 Convolution**을 통해 선형 변환됩니다. 이는 각 입력의 채널(feature) 차원을 `F_int`로 맞춰주고, 정보를 압축/변형하는 역할을 합니다.
    * **덧셈 (Addition):** 변환된 두 특징 맵(`W_x * x^l` 와 `W_g * g`)을 **요소별로(element-wise) 더합니다**. 이 과정에서 `x^l`의 지역적 특징과 `g`의 문맥 정보가 결합됩니다. (덧셈 전에 두 특징 맵의 공간적 크기(H, W, D)가 같도록 조정됨. g의 형태에 맞게 H_g W_g D_g로 통일)
    * **ReLU activation (`σ₁`):** 합쳐진 특징 맵에 **ReLU(Rectified Linear Unit)** 활성화 함수를 적용합니다. 이는 음수 값을 0으로 만들어 비선형성(non-linearity)을 추가하고, 유의미한 양수 신호만 통과시킵니다.
    * **선형 변환 (`ψ`):** ReLU를 통과한 특징 맵에 다시 **1x1x1 Convolution**(`ψ`)을 적용합니다. 채널 차원을 1로 줄여, 각 위치(pixel/voxel)별로 단일한 'attention score'를 계산하기 위한 전 단계입니다.
    * **Sigmoid activation (`σ₂`):** `ψ`를 통과한 결과에 **Sigmoid** 활성화 함수를 적용합니다. Sigmoid 함수는 모든 값을 **0과 1 사이**로 압축시킵니다. 이 결과가 바로 **Attention Coefficient (α)** 맵입니다.
      * 값이 1에 가까울수록 해당 위치가 중요하다는 의미이고, 0에 가까울수록 중요하지 않다는 의미입니다.
    * **리샘플러 (Resampler):** `α` 맵은 게이팅 신호 `g`와 동일한 공간 해상도(`H_g x W_g x D_g`)를 가집니다. 이를 원래 입력 특징 `x^l`의 해상도(`H_x x W_x x D_x`)에 적용하기 위해, **trilinear interpolation**을 사용하여 `α` 맵의 크기를 `x^l`과 동일하게 확대(upsampling)합니다. 
    * **곱셈 (Multiplication):** 리샘플링된 어텐션 계수(`α`) 맵을 원래의 입력 특징 `x^l`에 **요소별로 곱합니다**. 이 과정을 통해, 중요하다고 판단된 영역(α ≈ 1)의 특징은 그대로 유지되고, 중요하지 않은 영역(α ≈ 0)의 특징은 억제됩니다.
    * **출력 (`hat{x}^l`):** 최종적으로 어텐션 가중치가 적용된 특징 맵(`hat{x}^l`)이 출력됩니다. 이 출력은 U-Net 디코더에서 상위 레이어의 특징 맵과 결합(concatenation)되어 다음 단계로 전달됩니다.(figure 1)
![image](https://github.com/user-attachments/assets/288a8e0e-d9ba-438f-9acc-223674f1b033)


**요약:** Attention Gate는 상세 정보(`x^l`)와 문맥 정보(`g`)를 결합하여, 상세 정보 중에서 현재 작업(해당 논문의 경우, 췌장 이미지 분할)에 **중요한 부분만 골라내기 위한 'α map'을 생성**하고, 
이를 원래 상세 정보에 곱하여 **불필요한 정보는 걸러내고 중요한 정보만 강조**하는 필터 역할을 합니다.



관련 수식 설명 (Equations 1, 2, 3)

**Equation (1) & (2): 어텐션 계수(α) 계산** 

$$q_{att}^{l} = \psi^{T}(\sigma_{1}(W_{x}^{T}x_{i}^{l} + W_{g}^{T}g_{i} + b_{g})) + b_{\psi}$$
$$\alpha_{i}^{l} = \sigma_{2}(q_{att}^{l}(x_{i}^{l}, g_{i}; \Theta_{att}))$$

* **의미:** 이 두 식은 Figure 2에서 설명한 어텐션 계수 `α`를 계산하는 과정을 수식으로 나타낸 것입니다. (단, **리샘플링 전 단계까지**)
* **`x_i^l`**: 레이어 `l`의 공간적 위치 `i`에서의 입력 특징 벡터 (Figure 2의 `x^l`의 한 픽셀/복셀에 해당)
* **`g_i`**: 공간적 위치 `i`에 해당하는 게이팅 신호 벡터 (Figure 2의 `g`의 한 픽셀/복셀에 해당)
* **`W_x^T`, `W_g^T`, `ψ^T`**: 각각 `x_i^l`, `g_i`, 그리고 ReLU 결과에 적용되는 선형 변환(1x1x1 컨볼루션 가중치 행렬). `T`는 학습 가능한 변환 파라미터(가중치)를 나타내는 듯?
* **`b_g`, `b_ψ`**: 각 선형 변환 후 더해지는 편향(bias) 벡터/스칼라.
* **`σ₁`**: ReLU 활성화 함수 (수식에서는 첫 번째 활성화 함수).
* **`σ₂`**: Sigmoid 활성화 함수 (수식에서는 두 번째 활성화 함수).
* **`q_att^l`**: 'attention score'. `x`와 `g`의 정보를 결합하고 ReLU를 통과시킨 후, Sigmoid 직전의 값입니다. 이 값이 클수록 해당 위치에 더 많은 주의를 기울여야 함을 모델에 전달합니다.
* **`α_i^l`**: 최종 어텐션 계수. 위치 `i`의 특징이 얼마나 중요한지를 나타내는 0과 1 사이의 값입니다.
* **`Θ_att`**: Attention Gate 내부의 모든 학습 가능한 파라미터(`W_x`, `W_g`, `ψ`, `b_g`, `b_ψ`) 집합. 이 파라미터들은 모델 학습 과정에서 최적화됩니다.

**Equation (3): 역전파 시 Attention Gate의 역할**

$$\dfrac{\partial (\hat{x}_i^l)}{\partial \, (\Phi^{l-1})} = \dfrac{\partial \left(\alpha_i^l \, f(x_i^{l-1}; \Phi^{l-1})\right)}{\partial \, (\Phi^{l-1})} = \alpha_i^l \, \dfrac{\partial(f(x_i^{l-1}; \Phi^{l-1}))}{\partial \, (\Phi^{l-1})} + \dfrac{\partial (\alpha_i^l)}{\partial \, (\Phi^{l-1})} \, x_i^l$$

이 수식은 Attention Gate의 출력값인 $\hat{x}_i^l$이 이전 레이어($l-1$)의 파라미터($Φ^{l-1}$)에 대해 어떻게 변하는지(미분)를 나타냅니다. 
* 즉, **역전파(Backpropagation)** 시 그래디언트(Gradient)를 계산하는 것입니다. 즉, Attention Gate를 통과한 정보가 이전 레이어의 학습(파라미터 업데이트)에 어떤 영향을 미치는지를 수학적으로 나타낸 것입니다.

각 항을 자세히 살펴보겠습니다.

1.  **맨 왼쪽: $\dfrac{∂(\hat{x}_{i}^{l})} {∂(Φ^{l-1})}$**
    * $\hat{x}^{l}_{i}$ : Attention Gate의 최종 출력 특징 맵에서 위치 $i$의 값입니다.
      * 단, $\hat{x}_ {i}^{l} = α_{i}^{l} * x_{i}^{l}$
    * $Φ^(l-1)$: 레이어 $l-1$의 학습 가능한 파라미터(가중치, 편향 등) 집합입니다. 이 레이어는 $x^l$ (Attention Gate에 입력되기 전의 특징 맵)을 생성합니다.
      * **맨 왼쪽의 의미:** Attention Gate의 출력이 레이어 $l-1$의 파라미터 변화에 얼마나 민감하게 반응하는지를 나타내는 미분값(그래디언트)입니다.

2.  **중앙: $\dfrac{∂(α_i^l * f(x_i^{l-1}; Φ^{l-1}))} {∂(Φ^(l-1))}$**
    * $f(x_i^{l-1}; Φ^{l-1})$ : 레이어 {l-1}의 연산을 나타냅니다. 입력 $x_i^{l-1}$을 받아 파라미터 $Φ^{l-1}$를 사용하여 출력 $x_i^l$을 계산하는 함수입니다.
      * 즉, $x_i^l = f(x_i^{l-1}; Φ^{l-1})$ 입니다.
    * $α_i^l$: 위치 $i$에서의 어텐션 계수(0과 1 사이의 값).
      * **중앙의 의미:** 좌변의 $hat{x}_i^l$을 그 정의($α_i^l * x_i^l$)와 $x_i^l$의 정의($f(...)$)를 이용하여 풀어서 쓴 것입니다.

3.  **맨 오른쪽 (곱의 미분 법칙 적용): $\dfrac{α_i^l * ∂(f(x_i^{l-1}; Φ^{l-1}))} {∂(Φ^{l-1})} + \dfrac{∂(α_i^l)} {∂(Φ^{l-1}) * x_i^l}$**
    * 여기서 **곱의 미분 법칙 $\dfrac{d(uv)}{dx} = u(\dfrac{dv}{dx}) + v(\dfrac{du}{dx})$** 이 적용되었습니다.
    * $u = α_i^l$ (어텐션 계수)
    * $v = f(x_i^{l-1}; Φ^{l-1}) = x_i^l$ (레이어 $l-1$의 출력)
    * 미분 대상은 $Φ^{l-1}$ 입니다.

    * **첫 번째 항: $α_i^l * \dfrac{∂(f(x_i^{l-1}; Φ^{l-1}))}{∂(Φ^{l-1})}$**
        * $\dfrac{∂(f(x_i^{l-1}; Φ^{l-1}))} {∂(Φ^{l-1})}$ : 레이어 $l-1$의 출력($x_i^l$)이 해당 레이어의 파라미터($Φ^{l-1}$)에 대해 어떻게 변하는지를 나타내는 그래디언트입니다. 일반적인 CNN 역전파 과정에서 계산되는 항인듯.
        * **핵심:** 이 일반적인 그래디언트 항이 어텐션 계수인 $α_i^l$에 의해 **스케일링(scaling)** 됩니다. 즉, 어텐션 값이 높은($α_i^l ≈ 1$) 영역의 그래디언트는 거의 그대로 전달되지만, 어텐션 값이 낮은($α_i^l ≈ 0$) 영역의 그래디언트는 크게 감소합니다.
          * 논문에서는 "The first gradient term on the right-hand side is scaled with α" 라고 언급한 부분입니다. 이는 관련 없는 영역으로부터 오는 그래디언트 흐름을 억제하는 효과를 줄 듯...

    * **두 번째 항: $\dfrac{∂(α_i^l)} {∂(Φ^(l-1))} * x_i^l$**
        * $\dfrac{∂(α_i^l)} {∂(Φ^(l-1))}$ : 어텐션 계수 $α_i^l$ 자체가 레이어 $l-1$의 파라미터 $Φ^{l-1}$` 변화에 얼마나 민감하게 반응하는지를 나타냅니다.
          * 왜냐하면 $α_i^l$은 $x_i^l$ (그리고 게이팅 신호 $g$)에 의존하고, $x_i^l$은 다시 $Φ^{l-1}$에 의존하기 때문입니다. 즉, $Φ^{l-1}$가 변하면 $x_i^l$이 변하고, 이는 다시 $α_i^l$의 계산에 영향을 줄 수 있습니다.
        * **의미:** 이 항은 레이어 `l-1`의 파라미터가 **어텐션 계수 자체의 계산에 미치는 영향**을 통해 발생하는 그래디언트 경로를 나타냅니다. 즉, 파라미터 $Φ^{l-1}$가 어텐션 값을 '잘못된' 방향으로 변화시키는 경향이 있다면, 이 항을 통해 해당 파라미터가 수정될 수 있습니다.

수식 3은 Attention Gate를 통과하는 그래디언트가 어떻게 계산되는지를 보여줍니다. 이 그래디언트는 두 부분으로 나뉩니다.

1.  **기존 그래디언트의 스케일링:** 일반적인 역전파 그래디언트가 어텐션 계수 $α_i^l$에 의해 조절됩니다. 이는 학습 과정에서 중요한 영역(높은 $α$)의 영향력은 유지하고 중요하지 않은 영역(낮은 $α$)의 영향력은 줄이는 핵심 메커니즘입니다.
2.  **어텐션 계수 변화에 대한 그래디언트:** 이전 레이어 파라미터가 어텐션 계수 값 자체에 미치는 영향을 반영하는 그래디언트입니다.

결과적으로, Attention Gate는 역전파 과정에서 그래디언트 흐름을 동적으로 제어하여, 모델이 현재 작업과 관련된 특징 학습에 더 집중하도록 유도합니다.
